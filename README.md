# AI_pipeline

The base caller and cacher of the NN pipeline for the API created in a joint effort between BSEC and IMS.

The primary practical purpose of this module is to manage all other submodules.  Specifically it will determine the order and identify the cache so the model will build and predict in a dependable and repetable atmosphere.

## Quickstart Guide

### Initial Setup
Clone the repo
```
git clone https://ncigitlab01.repd.ornlkdi.org/schaefferknt/modularization_pipeline.git
```
Load the working branch and pull in the subrepos
```
cd modularization_pipeline
git checkout dev_branch
git submodule update --init --recursive
```
The conda environment has been saved in the scratch directory and doesn't need to be installed.
```
cd documentation
conda env create
```
Activate your conda environment
```
conda activate modular
```

### Data Generation
Setup data_args.json
```
python run_pipeline.py --write_default_config
```
The data_args.json controls the settings for data generation, information for each entry can be found in documentation/arg_descriptions (note that this file is only generated after running `python run_pipeline.py --write_default_config`). The default data_args.json file will generate the data splits used in the path-coding leaderboard, i.e., site/subsite/laterality/histology/behavior for LA, KY, UT, NJ, and SA. Additional useful data_arg settings are shown below:

Fast prototyping with ~100K records (restricts to only CTCs with a single report):
```
"Parser": "db_list": ["LA6", "KY5"]
"Filter": "only_single": "case"
```
Reportability:
```
"Parser": "db_list": ["CA2_bin", "SA4_bin"]
"Filter": "tasks": ["reportability"]
"Filter": "window_days": [0]
"Encoder": "metadata_fields": ["registryId", "recordDocumentId", "patientId", "tumorId"]
"Filter": "exclude_filed_scores": {"reportability":["1","2"]}
"Data_Generator": "min_count": 5
```
Bucketing:
```
"Parser": "db_list": ["KY2_bin", "LA5_bin", "NJ5_bin", "UT4_bin"]
"Filter": "tasks": ["report_category"]
"Filter": "window_days": [0]
"Encoder": "metadata_fields": ["registryId", "recordDocumentId", "patientId", "tumorId"]
"Data_Generator": "min_count": 5
```
To generate data, make sure that output file **does not exist**, otherwise the job will break.
```
sbatch data_job.sh
```
By default, all data is generated to data/outputs/, this directory can be configured using the `"Pipeline":"data_output"` entry in data_args.json. The following data elements are generated:
- data_foldX.csv: A csv containing metadata, tokenized text (converted into integers), and labels
- id2labels_foldX.json: A mapping of integers to labels, used for model training
- id2word_foldX.json: A mapping of integers to words, can be used with data_foldX.csv to convert tokenized integers into text
- word_embeds_foldX.npy: A numpy array of pretrained word embeddings (or random embeddings if Word2Vec is disabled), which matches the mapping in id2word_foldX.json
- metadata.json, schema.json, query.txt: these are used when using the model to predict on new data

### Model Training
The model_args.yaml file controls the settings for model training. Edit this file as desired. The `"save_name"` entry controls the name used for model checkpoints and prediction outputs; if left empty, a datetimestamp will be used.

**By default there are two gpus used.**
To change this, edit the model_job.sh file.

To train the model, simply run
```
sbatch model_job.sh
```
The script will automatically save model checkpoints and generate several prediction csvs. Model checkpoints are saved to ./savedmodels and predictions are saved to ./predictions. The following predictions are generated:
- <save_name>_scores.csv: f1 micro and macro scores for train, validation, and test splits for each task
- <save_name>_predictions.csv: ground truth and predicted labels for each record, fold, and task
- <save_name>_<task>_probabilities.csv: probabilities assigned to each possible class for each record in a given task
- <save_name>_abs_stats.txt: abstention log if abstention is enabled

You can also use the following command to regenerate the score csv and prediction csvs of a model that has already been trained:
```
python model_suite.py -mp <path_to_saved_model>
```

In addition, as of APIv11, there is now an additional Case-Level-Context model that is trained as a second stage model that utilizes the outputs from the primary classification model (from model_suite.py). The clc_args.yaml file controls the settings for the case-level-context model training. Edit this file as desired. Note that the `"model_path"` entry must be a valid .h5 model checkpoint generated by model_suite.py.

To train the case-level-context model, run 
```
python case_level_context.py
```
### Notes On Predicting with use_model.py

- Unless the model_data option is used (not yet verified to work) the appropriate cache will be verified (and rebuilt) in the directory `<modularization_pipeline>/api_cache/`
  based on what has been stored in the model. This ignores `data_args.json` and any other data in the directory.
    - It is suggested that you copy the cache/data package that was used originally to save time.
- Every run will require the model to be initiated which takes time. This is not intended for multiple runs, but small scale verification. If you really want to avoid the
  API repo, then adjustments in the code will be necessary.
- Predicting with a single json report will verify the model predicts consistently. If softmaxes and predictions are needed, it is suggested that you create a query (noted below)
  and predict using the database method or the DOE-API repo with the client server setup.
- Outputs will be predictions and confidences saved to files.

arguments for the cli are as follows:
*Required*:
model_path              the relative path for the model that will be used

*Optional*
--json_report JSON_REPORT
    -jr JSON_REPORT
                        the relative path of a single report to be predicted on. This will take priority over -d. If this is not included, the option of -d is expected. (default: )
--dataset DATASET 
    -d DATASET
                        the path to the testing dataset that will be predicted on
                        (default: )
--model_data MODEL_DATA
    -md MODEL_DATA
                        path to where the pregenerated data is stored. If not included, the cache will be used/built (default: )
--general_query
    -gq                 will run a geneneral query on the dataset instead of the one saved in the data (default: False)
--filter_data
    -fd                 will filter the dataset pointed at with -d (default: False)
--test
    -t                  will test the model for consistency and speed if -jr option is used (default: False)

### Using a Trained Model to Predict on a Report
The use_model.py script can be used to predict on a single report. A pretrained model will be required as well as the pregenerated data for the model.
The pregenerated data can either be in the form of the cache (which could be copied or rebuilt) or the data package that is associated with the model (using the -md option).
For the new data, the script will use the same data preprocessing settings as those used in the trained model.

There are two enpoints for the json runs. Both produce the saved predictions and confidences for the report provided. If `-t` is included it will also produce a testing of the model: differences in the predictions, softmaxes, and an average time for running the report. These printouts should be 0, 0 and <100 (or <10 ideally) respectively.

The report (in JSON form) will have to have to include a text field (i.e. textPathClinicalHistory, textStagingParams, textPathMicroscopicDesc, textPathNatureOfSpecimens, 
textPathSuppReportsAddenda, textPathFormalDx, textPathFullText, textPathComments, textPathGrossPathology) as well as both a patientId, tumorId, and recordDocumentId to run. An example report is: 
{"recordDocumentId":"REC-1234", "patientId":12345678, "tumorId":"01", "textPathClinicalHistory": "test text"}

```
python use_model.py <path_to_saved_model> -jr <path_to_json_report>
python use_model.py <path_to_saved_model> -jr <path_to_json_report> -md <path_to_data_package_the_model_was_trained_on> #saves time
```

### Using a Trained Model to Predict on a Database
The use_model.py script can be used to predict on a dataset. A pretrained model will be required as well as the pregenerated data for the model.
The pregenerated data can either be in the form of the cache (which could be copied or rebuilt) or the data package that is associated with the model (using the -md option).
For the new data, the script will use the same data preprocessing settings as those used in the trained model.


You can use the use_model.py script to use a trained model to predict on a dataset.
```
python use_model.py <path_to_saved_model> -d <path_to_sqlite_database_to_predict> -gq
python use_model.py <path_to_saved_model> -d <path_to_sqlite_database_to_predict> -gq -md <path_to_data_package_the_model_was_trained_on> #saves time
```
Below are a list of the latest sqlite database paths:
* CA1: /mnt/nci/scratch/data/CA/analysis_datasets/20200309_ctc/20210112_custom/raw.sqlite
* KY5: /mnt/nci/scratch/data/KY/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210806_03f879a5/raw.sqlite
* LA6: /mnt/nci/scratch/data/LA/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite
* NJ3: /mnt/nci/scratch/data/NJ/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite
* NM2: /mnt/nci/scratch/data/NM/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite
* SA7: /mnt/nci/scratch/data/SA/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite
* UT3: /mnt/nci/scratch/data/UT/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite
* CA1_bin (reportability): /mnt/nci/scratch/data/CA/analysis_datasets/20200309/20210112_custom/raw.sqlite
* SA2_bin (reportability): /mnt/nci/scratch/data/SA/analysis_datasets/20200402_0616/SA_bin_data.sqlite

This script will generate several prediction csvs in the modularization_pipeline/predictions directory.

Examples with a model saved at `../model_files/HISAN.h5`, json report saved at `./report.json` and using LA6 to predict:
```
python use_model.py ../model_files/HISAN.h5 -jr ./report.json
python use_model.py ../model_files/HISAN.h5 -d /mnt/nci/scratch/LA/analysis_datasets/20210518/20200407_dfd1b51fd72b/20210809_03f879a5/raw.sqlite -gq
```

### LIME Visualization

LIME is a tool that allows visualization of which words in a particular document a given model is utilizing to come to a classification decision. See the repo `https://ncigitlab01.repd.ornlkdi.org/sayeradbl/lime_explanations/` to use this capability (permission may need to be requested).

----------

## Modules
All moduels will be expected to contin both a test suite and a test cache so they can be run indipendently of the pipeline.

- Parser - Orgainizes data from raw formats (XML, JSON, HL7s, CSV) into robust databases and eventually refined dataframes.
- Cleaner - Formats the text reports into computer readable tokens and builds a vocabulary.
- Sanitizer - Converts the data into formats into more security friendly formats when necessary.
- Filter - Filters out data that is irrelevant
- Splitter - Converts records into the different splits (train, validation, and test).
- Data_Generator - Generate the correctly formatted input and output for the models.
- Encoder - Converts and saves the specific reports for use in the model.
- Model_Suite - Contains the model code: building, training, predicting, and evaluating.

# Tools
These will be used to ensure that the pipeline will run correctly and can be repeated.
## Independent of the cache
- run_pipeline - for activating the pipeline and build a model
- caching - manages ordering of the submodules and produces a class to be passed to each module based on dependencies, arguements (including environmental args), and commits. The purpose of the class is to send and retrieve stored data correctly and repeatedly.
- arg_distribution - for building the args for each of the submodule
- quick_predict - for predicting without the front end server
- testing_suite - as with all the modules, the test suite will include sanity checks and bug identification to ensure that the various tools meet expectations.
## Dependent on the cache
- documentation - for compiling a summary of the pipeline
- scriptor - for compiling a usable script to regenerate the model from scratch

## Expected Documentation
The following files will be for documentation (located in the `documentation` directory) and will also be used in the pipeline to ensure that everything is cached as expected. They will be recreated with each run of the respective tool if possible.

- deps.json
- environment.yml (will need to be manually created)
- arg_description.yml
- predictions.csv
- test_results.md
- model_desc.md
- model_script.py


## Run commands
All run commands will be expected to have defaults of the "official" API and should be one line calls.  All variables included in the call line is required to be included in the summary before anything else.  The totality of the calls should be as simple as:

```
python run_pipeline.py
python caching.py
python quick_predict.py
python testing_suite.py
python scriptor.py
```

# Creating new modules
Three steps:
## 1. include the new module in the run\_pipeline.py

In the script `run_pipeline.py` the intruduction of `call_order` requires the name of the module. 
Currenlty this is on line 40 of the script. If the example is followed this needs to occur ofter the Splitter in the sequence since the Splitter is referenced.

## 2. include the module at the base level

An example module (`Temp_mod`) is located in the documentaiton directory. As shown in the `Temp_mod` there are requirements
* the directory name needs to match that which is in `run_pipeline.py` (capitalization matters).
* the script name needs to match that whcih is in the `run_pipeline.py` (lowercase)
* the class name needs to match that of the directory and the `run_pipeline.py` (capitalization matters).
* the script has some examples of how to use it, but all functions (except the temp\_scirpt) are expected.
* the module must be the local version of a git repo for the cache to produce correctly.

## 3. the core scripts are expected to be updated

* deps.json - the data_args called in the new module needs to be included
* use_model.py - check to ensure the description is called from the last module.
* generate_data.py - ensure the pipeline data is being correctly output to the data directory.

The module (a template is located in `./documentation/temp_mod` needs to be included in the base directory
